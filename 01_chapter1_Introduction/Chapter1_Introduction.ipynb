{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning - Chapter 1\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we’ll implement a simple neural network to classify handwritten digits from the MNIST dataset. This is based on **Chapter 1** of Michael Nielsen’s book *\"Neural Networks and Deep Learning\"*. We’ll break down the code into small, understandable steps and provide explanations for each part.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Libraries\n",
    "First, let’s import the necessary libraries. We’ll use `numpy` for numerical computations and `random` for shuffling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define the Network Class\n",
    "\n",
    "**Explanation**:\n",
    "The `Network` class is the heart of our neural network. It’s like a factory that builds and trains the network. Here’s what it does:\n",
    "- **Initialization**: Sets up the network with random weights and biases. Think of weights as \"knobs\" that the network adjusts to learn, and biases as \"offsets\" that help fine-tune the output.\n",
    "- **Layers**: The network has multiple layers: an input layer, one or more hidden layers, and an output layer. For example, `[784, 30, 10]` means:\n",
    "  - Input layer: 784 neurons (one for each pixel in a 28x28 image).\n",
    "  - Hidden layer: 30 neurons (the \"brain\" of the network).\n",
    "  - Output layer: 10 neurons (one for each digit class, 0 through 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"Initialize the network with the given layer sizes.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Feedforward Method\n",
    "\n",
    "**Explanation**:\n",
    "The `feedforward` method is how the network makes predictions. It’s like a conveyor belt:\n",
    "1. Takes an input (e.g., an image of a digit).\n",
    "2. Passes it through each layer of the network.\n",
    "3. Applies the sigmoid activation function to \"squash\" the output into a range between 0 and 1.\n",
    "4. Returns the final output, which represents the network’s prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(self, a):\n",
    "        \"\"\"Compute the output of the network for input `a`.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Explanation**:\n",
    "Training a neural network is like teaching a child to ride a bike. You show them examples, and they learn from their mistakes. Here’s how it works:\n",
    "1. **Mini-Batches**: Instead of learning from all the data at once, the network learns from small chunks (mini-batches). This makes training faster and more efficient.\n",
    "2. **Epochs**: The network goes through the entire dataset multiple times (epochs) to improve its performance.\n",
    "3. **Learning Rate (`eta`)**: Controls how big the steps are when adjusting the weights and biases. Too big, and the network might overshoot; too small, and it might take forever to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"Train the network using stochastic gradient descent.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            time1 = time.time()\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            time2 = time.time()\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}, took {3:.2f} seconds\".format(\n",
    "                    j, self.evaluate(test_data), n_test, time2-time1))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete in {1:.2f} seconds\".format(j,  time2-time1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Update Mini-Batch\n",
    "\n",
    "**Explanation**:\n",
    "The `update_mini_batch` method is where the magic happens! It’s like a coach correcting the network’s mistakes:\n",
    "1. **Backpropagation**: Computes how much each weight and bias contributed to the error.\n",
    "2. **Gradient Descent**: Adjusts the weights and biases to reduce the error.\n",
    "3. **Learning Rate (`eta`)**: Determines how much to adjust the weights and biases. Think of it as the \"step size\" in the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases using backpropagation on a mini-batch.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                        for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Backpropagation\n",
    "\n",
    "**Explanation**:\n",
    "Backpropagation is the \"brain\" of the learning process. It’s like solving a mystery:\n",
    "1. **Forward Pass**: Computes the output of the network for a given input.\n",
    "2. **Error Calculation**: Compares the output to the true label to compute the error.\n",
    "3. **Backward Pass**: Propagates the error backward through the network to compute gradients for each weight and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self, x, y):\n",
    "        \"\"\"Compute the gradients for the cost function using backpropagation.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # Feedforward\n",
    "        activation = x\n",
    "        activations = [x]  # List to store all the activations, layer by layer\n",
    "        zs = []  # List to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # Backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate the Network\n",
    "\n",
    "**Explanation**:\n",
    "The `evaluate` method checks how well the network is performing. It’s like a teacher grading a test:\n",
    "1. **Predictions**: The network makes predictions for the test data.\n",
    "2. **Accuracy**: Compares the predictions to the true labels and calculates the percentage of correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(self, test_data):\n",
    "        \"\"\"Evaluate the network's performance on the test data.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Cost Function Derivative\n",
    "\n",
    "**Explanation**:\n",
    "The `cost_derivative` method computes how much the output differs from the true label. It’s like measuring how far off the network’s prediction is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Compute the derivative of the cost function.\"\"\"\n",
    "        return (output_activations - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Sigmoid Activation Function\n",
    "\n",
    "**Explanation**:\n",
    "The `sigmoid` function is the activation function used in the network. It’s like a \"squashing\" function that maps any input to a value between 0 and 1. This helps the network make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Sigmoid Derivative\n",
    "\n",
    "**Explanation**:\n",
    "The `sigmoid_prime` function is the derivative of the sigmoid function. It’s used in backpropagation to compute how much to adjust the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Load the MNIST Dataset\n",
    "\n",
    "**Explanation**:\n",
    "The MNIST dataset is a collection of 70,000 images of handwritten digits (0 through 9). Each image is 28x28 pixels, and we’ll preprocess the data to make it ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist_loader\n\u001b[1;32m      2\u001b[0m training_data, validation_data, test_data \u001b[38;5;241m=\u001b[39m mnist_loader\u001b[38;5;241m.\u001b[39mload_data_wrapper()\n\u001b[1;32m      3\u001b[0m net \u001b[38;5;241m=\u001b[39m Network([\u001b[38;5;241m784\u001b[39m, \u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Train the Network\n",
    "\n",
    "**Explanation**:\n",
    "Now it’s time to train the network! We’ll use the MNIST dataset to teach the network how to recognize handwritten digits. The network will learn by adjusting its weights and biases to minimize the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 20,20,10])\n",
    "net.SGD(training_data, 30, 10, 0.1, lmbda = 5.0,evaluation_data=validation_data, \n",
    "    monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Visualize Predictions\n",
    "\n",
    "**Explanation**:\n",
    "Let’s see how well the network is doing! We’ll visualize some test images along with the network’s predictions and the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot some test images and their predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    x, y = X_test[i], y_test[i]\n",
    "    prediction = np.argmax(net.feedforward(x))\n",
    "    ax.imshow(x.reshape(28, 28), cmap='gray')\n",
    "    ax.set_title(f\"Pred: {prediction}, True: {np.argmax(y)}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we implemented a simple feedforward neural network to classify handwritten digits from the MNIST dataset. The network achieved reasonable accuracy, demonstrating the power of even basic neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "1. Experiment with different network architectures (e.g., more layers, more neurons).\n",
    "2. Try different activation functions (e.g., ReLU).\n",
    "3. Add regularization techniques like dropout or L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
