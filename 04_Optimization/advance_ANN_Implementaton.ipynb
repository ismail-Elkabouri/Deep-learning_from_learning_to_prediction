{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Neural Networks and Deep Learning: From Theory to Implementation\n",
    "\n",
    "This Jupyter Notebook is based on the book **[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap3.html)** by **Michael Nielsen**. It provides a hands-on guide to understanding and implementing neural networks from scratch. The notebook covers key concepts such as **weight initialization**, **cost functions**, **gradient descent**, and **regularization**.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)  \n",
    "2. [Weight Initialization](#weight-initialization)  \n",
    "3. [Cost Functions](#cost-functions)  \n",
    "4. [Gradient Descent and Backpropagation](#gradient-descent-and-backpropagation)  \n",
    "5. [Regularization](#regularization)  \n",
    "6. [Training a Neural Network](#training-a-neural-network)  \n",
    "7. [Conclusion](#conclusion)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "Neural networks are a powerful tool for solving complex problems in machine learning. In this notebook, we will implement a neural network from scratch using Python and NumPy. We will focus on the following key concepts:\n",
    "\n",
    "- **Weight Initialization**: How to initialize weights and biases for optimal training.\n",
    "- **Cost Functions**: Understanding cross-entropy and quadratic cost functions.\n",
    "- **Gradient Descent**: Implementing the backpropagation algorithm to update weights and biases.\n",
    "- **Regularization**: Using L2 regularization to prevent overfitting.\n",
    "\n",
    "Let's get started by importing the necessary libraries.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Weight Initialization <a id=\"weight-initialization\"></a>\n",
    "\n",
    "Proper weight initialization is crucial for training neural networks. We will implement two initialization methods:\n",
    "\n",
    "1. **Default Weight Initializer**: Initializes weights using a Gaussian distribution with mean 0 and standard deviation \\(1/\\sqrt{n}\\), where \\(n\\) is the number of connections to the neuron.\n",
    "2. **Large Weight Initializer**: Initializes weights using a Gaussian distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "```python\n",
    "class Network:\n",
    "    def __init__(self, sizes, cost):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost = cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize weights and biases using the default method.\"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x) \n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize weights and biases using the large method.\"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Cost Functions <a id=\"cost-functions\"></a>\n",
    "\n",
    "We will implement two cost functions:\n",
    "\n",
    "1. **Quadratic Cost**: Measures the squared difference between the predicted and actual outputs.\n",
    "2. **Cross-Entropy Cost**: Measures the difference between the predicted and actual outputs using logarithms.\n",
    "\n",
    "```python\n",
    "class QuadraticCost:\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the quadratic cost for output `a` and desired output `y`.\"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta for the output layer.\"\"\"\n",
    "        return (a - y) * sigmoid_prime(z)\n",
    "\n",
    "class CrossEntropyCost:\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cross-entropy cost for output `a` and desired output `y`.\"\"\"\n",
    "        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta for the output layer.\"\"\"\n",
    "        return (a - y)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Gradient Descent and Backpropagation <a id=\"gradient-descent-and-backpropagation\"></a>\n",
    "\n",
    "Gradient descent is used to minimize the cost function by updating the weights and biases. Backpropagation is the algorithm used to compute the gradients.\n",
    "\n",
    "```python\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def backprop(self, x, y):\n",
    "    \"\"\"Return the gradient for the cost function C_x.\"\"\"\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    # Feedforward\n",
    "    activation = x\n",
    "    activations = [x]\n",
    "    zs = []\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        z = np.dot(w, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "    \n",
    "    # Backward pass\n",
    "    delta = self.cost.delta(zs[-1], activations[-1], y)\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    \n",
    "    for l in range(2, self.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoid_prime(z)\n",
    "        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "    \n",
    "    return nabla_b, nabla_w\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Regularization <a id=\"regularization\"></a>\n",
    "\n",
    "Regularization is used to prevent overfitting. We will implement **L2 regularization**, which adds a penalty term to the cost function based on the magnitude of the weights.\n",
    "\n",
    "```python\n",
    "def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "    \"\"\"Update the network's weights and biases using gradient descent.\"\"\"\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    for x, y in mini_batch:\n",
    "        delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "        nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "    \n",
    "    self.weights = [(1 - eta * (lmbda / n)) * w - (eta / len(mini_batch)) * nw \n",
    "                    for w, nw in zip(self.weights, nabla_w)]\n",
    "    self.biases = [b - (eta / len(mini_batch)) * nb \n",
    "                   for b, nb in zip(self.biases, nabla_b)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Training a Neural Network <a id=\"training-a-neural-network\"></a>\n",
    "\n",
    "We will now train a neural network on the MNIST dataset. The network will have one hidden layer with 30 neurons.\n",
    "\n",
    "```python\n",
    "# Load the MNIST dataset\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "# Initialize the network\n",
    "net = Network([784, 30, 10], cost=CrossEntropyCost)\n",
    "\n",
    "# Train the network\n",
    "net.SGD(training_data, epochs=30, mini_batch_size=10, eta=0.5, lmbda=5.0, \n",
    "        evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusion <a id=\"conclusion\"></a>\n",
    "\n",
    "In this notebook, we implemented a neural network from scratch and trained it on the MNIST dataset. We covered key concepts such as weight initialization, cost functions, gradient descent, and regularization. This implementation provides a solid foundation for understanding how neural networks work and can be extended to more complex architectures.\n",
    "\n",
    "For further reading, check out the book **[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap3.html)** by **Michael Nielsen**.\n",
    "\n",
    "---\n",
    "\n",
    "**References**:\n",
    "- Michael Nielsen, *Neural Networks and Deep Learning*, 2015.\n",
    "- [Deep Learning Course by Andrew Ng](https://www.coursera.org/learn/machine-learning)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is designed to be interactive and educational. You can run each code cell to see the results and experiment with different parameters. Let me know if you'd like further enhancements! ðŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
