{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```markdown\n",
    "# Convolutional Neural Networks (CNNs): From Theory to Implementation\n",
    "\n",
    "This Jupyter Notebook is based on the book **[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap6.html)** by **Michael Nielsen**. It provides a hands-on guide to understanding and implementing **Convolutional Neural Networks (CNNs)** using **Theano**. The notebook covers key concepts such as **convolutional layers**, **pooling layers**, **dropout**, and **training CNNs**.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)  \n",
    "2. [Convolutional Layers](#convolutional-layers)  \n",
    "3. [Pooling Layers](#pooling-layers)  \n",
    "4. [Dropout](#dropout)  \n",
    "5. [Training a CNN](#training-a-cnn)  \n",
    "6. [Conclusion](#conclusion)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction <a id=\"introduction\"></a>\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a powerful tool for solving complex problems in computer vision, such as image classification, object detection, and more. In this notebook, we will implement a CNN from scratch using **Theano**, a Python library for efficient numerical computation. We will focus on the following key concepts:\n",
    "\n",
    "- **Convolutional Layers**: Extracting features from images using convolutional filters.\n",
    "- **Pooling Layers**: Reducing the spatial dimensions of the feature maps.\n",
    "- **Dropout**: Preventing overfitting by randomly dropping neurons during training.\n",
    "- **Training CNNs**: Using stochastic gradient descent (SGD) to optimize the network.\n",
    "\n",
    "Let's get started by importing the necessary libraries.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv\n",
    "from theano.tensor.signal import downsample\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Convolutional Layers <a id=\"convolutional-layers\"></a>\n",
    "\n",
    "Convolutional layers are the building blocks of CNNs. They apply a set of filters to the input image to extract features such as edges, textures, and patterns.\n",
    "\n",
    "```python\n",
    "class ConvPoolLayer:\n",
    "    def __init__(self, filter_shape, image_shape, poolsize=(2, 2), activation_fn=T.nnet.sigmoid):\n",
    "        \"\"\"\n",
    "        Initialize a convolutional + pooling layer.\n",
    "        \n",
    "        :param filter_shape: Tuple of (number of filters, num input feature maps, filter height, filter width)\n",
    "        :param image_shape: Tuple of (mini-batch size, num input feature maps, image height, image width)\n",
    "        :param poolsize: Tuple of (pooling height, pooling width)\n",
    "        :param activation_fn: Activation function to use (e.g., sigmoid, ReLU)\n",
    "        \"\"\"\n",
    "        self.filter_shape = filter_shape\n",
    "        self.image_shape = image_shape\n",
    "        self.poolsize = poolsize\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        n_out = (filter_shape[0] * np.prod(filter_shape[2:]) / np.prod(poolsize))\n",
    "        self.W = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(loc=0, scale=np.sqrt(1.0 / n_out), size=filter_shape),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            np.asarray(\n",
    "                np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
    "        \"\"\"Set the input to the layer and compute the output.\"\"\"\n",
    "        self.inpt = inpt.reshape(self.image_shape)\n",
    "        conv_out = conv.conv2d(\n",
    "            input=self.inpt, filters=self.W, filter_shape=self.filter_shape, image_shape=self.image_shape\n",
    "        )\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out, ds=self.poolsize, ignore_border=True\n",
    "        )\n",
    "        self.output = self.activation_fn(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "        self.output_dropout = self.output  # No dropout in convolutional layers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Pooling Layers <a id=\"pooling-layers\"></a>\n",
    "\n",
    "Pooling layers reduce the spatial dimensions of the feature maps, making the network more computationally efficient and less prone to overfitting.\n",
    "\n",
    "```python\n",
    "# Pooling is integrated into the ConvPoolLayer class above.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Dropout <a id=\"dropout\"></a>\n",
    "\n",
    "Dropout is a regularization technique that randomly drops neurons during training to prevent overfitting.\n",
    "\n",
    "```python\n",
    "def dropout_layer(layer, p_dropout):\n",
    "    \"\"\"Apply dropout to a layer.\"\"\"\n",
    "    srng = theano.tensor.shared_randomstreams.RandomStreams(\n",
    "        np.random.RandomState(0).randint(999999)\n",
    "    )\n",
    "    mask = srng.binomial(n=1, p=1 - p_dropout, size=layer.shape)\n",
    "    return layer * T.cast(mask, theano.config.floatX)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Training a CNN <a id=\"training-a-cnn\"></a>\n",
    "\n",
    "We will now train a CNN on the MNIST dataset. The network will consist of convolutional, pooling, and fully connected layers.\n",
    "\n",
    "```python\n",
    "class Network:\n",
    "    def __init__(self, layers, mini_batch_size):\n",
    "        \"\"\"Initialize the network with a list of layers.\"\"\"\n",
    "        self.layers = layers\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.params = [param for layer in self.layers for param in layer.params]\n",
    "        self.x = T.matrix(\"x\")\n",
    "        self.y = T.ivector(\"y\")\n",
    "        \n",
    "        # Set up the network\n",
    "        init_layer = self.layers[0]\n",
    "        init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
    "        for j in range(1, len(self.layers)):\n",
    "            prev_layer, layer = self.layers[j - 1], self.layers[j]\n",
    "            layer.set_inpt(prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
    "        self.output = self.layers[-1].output\n",
    "        self.output_dropout = self.layers[-1].output_dropout\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, validation_data, test_data, lmbda=0.0):\n",
    "        \"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
    "        training_x, training_y = training_data\n",
    "        validation_x, validation_y = validation_data\n",
    "        test_x, test_y = test_data\n",
    "        \n",
    "        # Define the cost function and gradients\n",
    "        l2_norm_squared = sum([(layer.W**2).sum() for layer in self.layers])\n",
    "        cost = self.layers[-1].cost(self) + 0.5 * lmbda * l2_norm_squared / len(training_data)\n",
    "        grads = T.grad(cost, self.params)\n",
    "        updates = [(param, param - eta * grad) for param, grad in zip(self.params, grads)]\n",
    "        \n",
    "        # Define functions for training and evaluation\n",
    "        train_mb = theano.function(\n",
    "            [T.lscalar()], cost, updates=updates,\n",
    "            givens={\n",
    "                self.x: training_x[T.lscalar() * mini_batch_size: (T.lscalar() + 1) * mini_batch_size],\n",
    "                self.y: training_y[T.lscalar() * mini_batch_size: (T.lscalar() + 1) * mini_batch_size]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Train the network\n",
    "        best_validation_accuracy = 0.0\n",
    "        for epoch in range(epochs):\n",
    "            for minibatch_index in range(len(training_data) // mini_batch_size):\n",
    "                cost_ij = train_mb(minibatch_index)\n",
    "            # Evaluate on validation data\n",
    "            validation_accuracy = np.mean([\n",
    "                self.layers[-1].accuracy(self.y, \n",
    "                givens={\n",
    "                    self.x: validation_x[i * mini_batch_size: (i + 1) * mini_batch_size],\n",
    "                    self.y: validation_y[i * mini_batch_size: (i + 1) * mini_batch_size]\n",
    "                }) for i in range(len(validation_data) // mini_batch_size)\n",
    "            ])\n",
    "            print(f\"Epoch {epoch}: Validation Accuracy {validation_accuracy:.2%}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Conclusion <a id=\"conclusion\"></a>\n",
    "\n",
    "In this notebook, we implemented a Convolutional Neural Network (CNN) from scratch using **Theano**. We covered key concepts such as convolutional layers, pooling layers, dropout, and training CNNs. This implementation provides a solid foundation for understanding how CNNs work and can be extended to more complex architectures.\n",
    "\n",
    "For further reading, check out the book **[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap6.html)** by **Michael Nielsen**.\n",
    "\n",
    "---\n",
    "\n",
    "**References**:\n",
    "- Michael Nielsen, *Neural Networks and Deep Learning*, 2015.\n",
    "- [Theano Documentation](http://deeplearning.net/software/theano/)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is designed to be interactive and educational. You can run each code cell to see the results and experiment with different parameters. Let me know if you'd like further enhancements! 😊"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
